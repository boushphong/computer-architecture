A Computer System consists of hardware and systems software that work together to run application programs.

``` --hello.c
#include <stdio.h> 
int main()
{
    printf("hello, world\n");
    return 0;
}
```

## 1.1 Information is Bits + Context

![image](https://user-images.githubusercontent.com/59940078/192087453-f31a4ece-c3b6-422c-a999-bdf41c962ec6.png)

The _hello_ program begins life as a source program (or source file) that the programmer creates with an editor and saves it as a text file called **hello.c**. The source program is a sequence of bits, each with a value of 0 or 1, organized in 8-bit chunks called bytes. Each byte represents some text character in the program.

Most computer systems represent text characters using the ASCII standard that represents each character with a unique byte-size integer value. The image above shows the ASCII representation of the **hello.c** program.

The **hello.c** program is stored in a file as sequence of bytes. Each byte has an interger value that corresponds to some character. For example, "#" represents the integer value of 35, "i" represents the integer value 105 and so on. Each text line is terminated by the invisible newline character "\n", which is represented by the integer value of 10. Files such as **hello.c** that consist exclusively of ASCII characters are known as text files. All other files are known as binary files.

The representation of **hello.c** illustrates a _fundamental idea_: All information in a system - including disk files, programs stored in memory, user data stored in memory, and data transferred across a network - is represented as a bunch of bits. The only thing that distinguishes different data objects is the context in whcih we view them. For example, in different contexts, the same sequence of bytes might represent an integerm floating-point number, character string or machine instruction.

## 1.2 Programs are translated by other programs into different forms

The **hello** program begins life as a high-level C program because it can be read and understood by human beings in that form. However, in order to run **hello.c** on the system, the individual C statements must be translated by other programs into a sequence of _low-level machine-language_ instructions. These instructions are then packaged in a form called an _executable object program_ and stored as a binary disk file. Object programs are also referred to as executable object files.

On a UNIX system, the  translation from the source file is performed by a compiler driver.

<img width="725" alt="image" src="https://user-images.githubusercontent.com/59940078/192132109-eb84aec2-fbfb-4439-a5fc-815f22304031.png">

linux> gcc -o hello hello.c

Here, the gcc compiler driver reads the source file **hello.c** and translates it into an executable file _hello_. The translation is performed in the sequence of four phases in Figure 1.3. The programs that perform the four phases (preprocessor, compiler, assembler, and linker) are known collectively as the _compilation system_.

- **_Preprocessing phase_**: The preprocessor (cpp) modifies the original C program according to directives that begin with the ‘#’ character. For example, the #include <stdio.h> command in line 1 of hello.c tells the preprocessor to read the contents of the system header file stdio.h and insert it directly into the program text. The result is another C program, typically with the .i suffix.

- **_Compilation phase_**: The compiler (cc1) translates the text file **hello.i** into the text file **hello.s**, which contains an _assembly-language program_. This program includes the following definition of function main. Each of lines 2–7 in the image below in this definition describes one low-level machine- language instruction in a textual form. Assembly language is useful because it provides a common output language for different compilers for different high-level languages. For example, C compilers and Fortran compilers both generate output files in the same assembly language.

![image](https://user-images.githubusercontent.com/59940078/192147536-747e5f74-211b-4f48-947b-0572c3d3eb77.png)

- **_Assembly phase_**: Next, the assembler (as) translates hello.s into machine-language instructions, packages them in a form known as a _relocatable object program_, and stores the result in the object file hello.o. This file is a binary file containing 17 bytes to encode the instructions for function main. If we were to view hello.o with a text editor, it would appear to be gibberish.

- **_Linking phase_**: Notice that our hello program calls the _printf_ function, which is part of the _standard C library_ provided by every C compiler. The _printf_ function resides in a separate precompiled object file called _printf.o_, which must somehow be merged with our **hello.o** program. The linker (ld) handles this merging. The result is the **hello** file, which is an executable object file (or simply _executable_) that is ready to be loaded into memory and executed by the system.

## 1.3 It Pays to Understand How Compilation Systems Work

Modern compilers are sophisticated tools that usually produce good code. As a programmer, we generally do not need to know the inner workings of the compiler in order to write efficient code. However, in order to make good coding decisions, we do need a basic understanding of machine-level code.

- **_Optimizing program performance_**:  For example, is a switch statement always more efficent than a sequence of if-else statements? How much overhead is incurred by a function call? Is a _while_ loop more efficient than a _for_ loop? Are pointer references more efficient than array indexes? Why does our loop run so much faster if we sum into a local variable instead of an argument that is passed by reference? How can a function run faster when we simply rearrange the parentheses in an arithmetic expression?

- **_Understanding link-time errors_**: Some of the most perplexing programming errors are related to the operation of the linker, especially when you are trying to build large software systems. For example, what does it mean when the linker reports that it cannot resolve a reference? What is the difference between a static variable and a global variable? What happens if you define two global variables in different C files with the same name? What is the difference between a static library and a dynamic library? Why does it matter what order we list libraries on the command line? And scariest of all, why do some linker-related errors not appear until run time?

- **_Avoiding security holes_**: For many years, _buffer overflow vulnerabilities_ have accounted for many of the security holes in network and Internet servers. These vulnerabilities exist because too few programmers understand the need to carefully restrict the quantity and forms of data they accept from untrusted sources. A first step in learning secure programming is to understand the con- sequences of the way data and control information are stored on the program stack.

## 1.4 Processors Read and Interpret Instructions Stored in Memory
### 1.4.1 Hardware Organization of a System
To understand what happens to our hello program when we run it, we need to understand the hardware organization of a typical system, which is shown in Figure 1.4 (modeled after the family of recent Intel systems).

![image](https://user-images.githubusercontent.com/59940078/192150698-991c3118-0f53-4b9b-9eff-1279352f2971.png)

- **_Buses_**: Running throughout the system is a collection of electrical conduits called _buses_ that carry bytes of information back and forth between the components. Buses are typically designed to transfer fixed-size chunks of bytes known as _words_. The number of bytes in a word (the _word size_) is a fundamental system parameter that varies across systems. Most machines today have word sizes of either 4 bytes (32 bits) or 8 bytes (64 bits).

- **_IO Devices_**: Input/output (I/O) devices are the system’s connection to the external world. Our example system has four I/O devices: a keyboard and mouse for user input, a display for user output, and a disk drive (or simply disk) for long-term storage of data and programs. Initially, the executable hello program resides on the disk.

    Each I/O device is connected to the I/O bus by either a _controller_ or an _adapter_. The distinction between the two is mainly one of packaging. Controllers are chip sets in the device itself or on the system’s main printed circuit board (often called the _motherboard_). An adapter is a card that plugs into a slot on the motherboard. Regardless, the purpose of each is to transfer information back and forth between the I/O bus and an I/O device.
    
- **_Main Memory_**: The _main memory_ is a temporary storage device that holds both a program and the data it manipulates while the processor is executing the program. Physically, main memory consists of a collection of _dynamic random access memory_ (DRAM) chips. Logically, memory is organized as a linear array of bytes, each with its own unique address (array index) starting at zero. In general, each of the machine instructions that constitute a program can consist of a variable number of bytes. The sizes of data items that correspond to C program variables vary according to type. For example, on an x86-64 machine running Linux, data of type short require 2 bytes, types _int_ and _float_ 4 bytes, and types _long_ and _double_ 8 bytes.

- **_Processor_**: 
The _central processing unit_ (CPU), or simply processor, is the engine that inter- prets (or _executes_) instructions stored in main memory. At its core is a word-size storage device (or _register_) called the _program counter_ (PC). At any point in time, the PC points at (contains the address of) some machine-language instruction in main memory.

    From the time that power is applied to the system until the time that the power is shut off, a processor repeatedly executes the instruction pointed at by the program counter and updates the program counter to point to the next instruction. A processor _appears_ to operate according to a very simple instruction execution model, defined by its _instruction set architecture_. In this model, instructions execute in strict sequence, and executing a single instruction involves performing a series of steps. The processor reads the instruction from memory pointed at by the program counter (PC), interprets the bits in the instruction, performs some simple operation dictated by the instruction, and then updates the PC to point to the next instruction, which may or may not be contiguous in memory to the instruction that was just executed.
    
    There are only a few of these simple operations, and they revolve around main memory, the _register_ file, and the _arithmetic/logic unit_ (ALU). The register file is a small storage device that consists of a collection of word-size registers, each with its own unique name. The ALU computes new data and address values. Here are some examples of the simple operations that the CPU might carry out at the request of an instruction:
    
    - _Load_: Copy a byte or a word from main memory into a register, overwriting the previous contents of the register.
        
    - _Store_: Copy a byte or a word from a register to a location in main memory, overwriting the previous contents of that location.
        
    - _Operate_: Copy the contents of two registers to the ALU, perform an arithmetic operation on the two words, and store the result in a register, overwriting the previous contents of that register.
        
    - _Jump_: Extract a word from the instruction itself and copy that word into the program counter (PC), overwriting the previous value of the PC.
    
    A processor appears to be a simple implementation of its in- struction set architecture, but in fact modern processors use far more complex mechanisms to speed up program execution. Thus, we can distinguish the processor’s instruction set architecture, describing the effect of each machine-code instruction, from its _microarchitecture_, describing how the processor is actually implemented.

### 1.4.2 Running the _hello_ Program

Initially, the shell program is executing its instructions, waiting for us to type a command. As we type the characters **./hello** at the keyboard, the shell program reads each one into a register and then stores it in memory, as shown in Figure 1.5.

When we hit the enter key on the keyboard, the shell knows that we have finished typing the command. The shell then loads the executable _hello_ file by executing a sequence of instructions that copies the code and data in the hello object file from disk to main memory. The data includes the string of characters hello, world\n that will eventually be printed out.

![image](https://user-images.githubusercontent.com/59940078/192152034-96bf7eca-82a7-450f-9b1c-ed888a105a44.png)

Using a technique known as _direct memory access_ (DMA, discussed in Chapter 6), the data travel directly from disk to main memory, without passing through the processor. This step is shown in Figure 1.6.

Once the code and data in the hello object file are loaded into memory, the processor begins executing the machine-language instructions in the hello program’s main routine. These instructions copy the bytes in the _hello, world\n_ string from memory to the register file, and from there to the display device, where they are displayed on the screen. This step is shown in Figure 1.7.

## 1.5 Caches Matter

An important lesson from this simple example is that a system spends a lot of time moving information from one place to another. The machine instructions in the hello program are originally stored on disk. When the program is loaded, they are copied to main memory. As the processor runs the program, instruc- tions are copied from main memory into the processor. Similarly, the data string _hello,world\n_, originally on disk, is copied to main memory and then copied from main memory to the display device. From a programmer’s perspective, much of this copying is overhead that slows down the “real work” of the program. Thus, a major goal for system designers is to make these copy operations run as fast as possible.

Because of physical laws, larger storage devices are slower than smaller storage devices. And faster devices are more expensive to build than their slower counterparts. For example, the disk drive on a typical system might be 1,000 times larger than the main memory, but it might take the processor 10,000,000 times longer to read a word from disk than from memory.

![image](https://user-images.githubusercontent.com/59940078/192152321-9824ec27-8528-4f70-8f0d-035687910f72.png)

![image](https://user-images.githubusercontent.com/59940078/192152338-422c388d-bd76-46d3-a63f-f203e2bf5126.png)

Similarly, a typical register file stores only a few hundred bytes of information, as opposed to billions of bytes in the main memory. However, the processor can read data from the register file almost 100 times faster than from memory. Even more troublesome, as semiconductor technology progresses over the years, this _processor–memory gap_ continues to increase. It is easier and cheaper to make processors run faster than it is to make main memory run faster.

To deal with the processor–memory gap, system designers include smaller, faster storage devices called _cache memories_ (or simply caches) that serve as temporary staging areas for information that the processor is likely to need in the near future. Figure 1.8 shows the cache memories in a typical system. An _L1 cache_ on the processor chip holds tens of thousands of bytes and can be accessed nearly as fast as the register file. A larger _L2 cache_ with hundreds of thousands to millions of bytes is connected to the processor by a special bus. It might take 5 times longer for the processor to access the _L2 cache_ than the _L1 cache_, but this is still 5 to 10 times faster than accessing the main memory. The L1 and L2 caches are implemented with a hardware technology known as _static random access memory_ (SRAM). Newer and more powerful systems even have three levels of cache: L1, L2, and L3. The idea behind caching is that a system can get the effect of both a very large memory and a very fast one by exploiting _locality_, the tendency for programs to access data and code in localized regions. By setting up caches to hold data that are likely to be accessed often, we can perform most memory operations using the fast caches.

![image](https://user-images.githubusercontent.com/59940078/192153617-93552dd3-0954-46ae-9c44-fe6eddf33589.png)

## 1.6 Storage Devices Form a Hierarchy

This notion of inserting a smaller, faster storage device (e.g., cache memory) between the processor and a larger, slower device (e.g., main memory) turns out to be a general idea. In fact, the storage devices in every computer system are organized as a _memory hierarchy_ similar to Figure 1.9. As we move from the top of the hierarchy to the bottom, the devices become slower, larger, and less costly per byte. The register file occupies the top level in the hierarchy, which is known as level 0 or L0. We show three levels of caching L1 to L3, occupying memory hierarchy levels 1 to 3. Main memory occupies level 4, and so on.

The main idea of a memory hierarchy is that storage at one level serves as a cache for storage at the next lower level. Thus, the register file is a cache for the L1 cache. Caches L1 and L2 are caches for L2 and L3, respectively. The L3 cache is a cache for the main memory, which is a cache for the disk. On some networked systems with distributed file systems, the local disk serves as a cache for data stored on the disks of other systems.

## 1.7 The Operating System Manages the Hardware
